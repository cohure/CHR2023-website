@inproceedings{chr2021:Ehrmanntraut,
    author    = {Anton Ehrmanntraut and Thora Hagen and Leonard Konle and Fotis Jannidis},
    title     = {Type- and Token-based Word Embeddings in the Digital Humanities},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper35.pdf},
    abstract  = {In the general perception of the NLP community, the new dynamic, context-sensitive, token-based embeddings from language models like BERT have replaced the older static, type-based embeddings like word2vec or fastText, due to their better performance. We can show that this is not the case for one area of applications for word embeddings: the abstract representation of the meaning of words in a corpus. This application is especially important for the Computational Humanities, for example in order to show the development of words or ideas. The main contribution of our papers are: 1) We offer a systematic comparison between dynamic and static embeddings in respect to word similarity. 2) We test the best method to convert token embeddings to type embeddings. 3) We contribute new evaluation datasets for word similarity in German. The main goal of our contribution is to make an evidence-based argument that research on static embeddings, which basically stopped after 2019, should be continued not only because it needs less computing power and smaller corpora, but also because for this specific set of applications their performance is on par with that of dynamic embeddings.}
}


@inproceedings{chr2021:Enevoldsen,
    author    = {Kenneth Enevoldsen and Lasse Hansen and Kristoffer L. Nielbo},
    title     = {DaCy: A Unified Framework for Danish NLP},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper24.pdf},
    abstract  = {Danish natural language processing (NLP) has in recent years obtained considerable improvements with the addition of multiple new datasets and models. However, at present, there is no coherent framework for applying state-of-the-art models for Danish. We present DaCy: a unified framework for Danish NLP built on and integrated with SpaCy. DaCy uses efficient multitask models which obtain state-of-the-art performance on named entity recognition, part-of-speech tagging, and dependency parsing. DaCy contains tools for easy integration of existing models such as for polarity, emotion, or subjectivity detection. In addition, we conduct a series of tests for biases and robustness of Danish NLP pipelines through data augmentation. DaCy large compares favorably and is especially robust to long input lengths and spelling variations and errors. All models except DaCy large display significant biases related to ethnicity while only Polyglot shows a significant gender bias. We argue that for languages with limited benchmark sets, data augmentation can be particularly useful for obtaining more realistic and fine-grained performance estimates. We provide a series of augmenters as a first step towards a more thorough evaluation of language models for low and medium resource languages and encourage further development.}
}


@inproceedings{chr2021:Cafiero,
    author    = {Florian Cafiero and Jean-Baptiste Camps},
    title     = {`Psyché' as a Rosetta Stone? Assessing Collaborative Authorship in the French 17th Century Theatre},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper51.pdf},
    abstract  = {During the 17th century, a significant number of collaborations emerged between playwrights, among which authors as famous as Pierre Corneille, Thomas Corneille or Molière, as well as Philippe Quinault or Jean Donneau de visé. The actual division of labour between authors can sometimes be deduced from historical documents, but is most of the time uncertain. In this paper, we try to address this question by using the information we got from one specific instance of collaboration:  Psyché (1671). We first try to assess the accuracy of the notice to the reader of the printed edition of the play, where each author's involvement is clearly claimed, using machine learning and ``rolling stylometry'' methodology. We then use the optimal parameters already applied to this play to analyse other collaborative works of the time, in particular cases of potential collaboration between Thomas Corneille and Jean Donneau de Visé in  Circé  and  L'Inconnu .}
}

@inproceedings{chr2021:Pagel,
    author    = {Janis Pagel and Nidhi Sihag and Nils Reiter},
    title     = {Predicting Structural Elements in German Drama},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper34.pdf},
    abstract  = {We address the challenge of enriching plain text dramas with predicted TEI/XML elements. We use a large corpus of dramas annotated with TEI information about act/scene changes, speaker changes, and stage directions, among others. On this data, we fine-tune a pre-trained BERT transformer model on several subtasks, like predicting stage directions vs. utterances. We show that the used architecture is able to predict the learned structural elements on unseen data for several settings and models.}
}

@inproceedings{chr2021:Gamba,
    author    = {Federica Gamba and Marco Passarotti and Paolo Ruffolo},
    title     = {More Data and New Tools. Advances in Parsing the Index Thomisticus Treebank},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper20.pdf},
    abstract  = {This paper investigates the recent advances in parsing the  Index Thomisticus  Treebank, which encompasses Medieval Latin texts by Thomas Aquinas. The research focuses on two types of variables. On the one hand, it examines the impact that a larger dataset has on the results of parsing; on the other hand, performances of new parsers are analysed with respect to less recent tools. Term of comparison to determine the effective parsing advances are the results in parsing the  Index Thomisticus  Treebank described in a previous work. First, the best performing parser among those concerned in that study is tested on a larger dataset than the one originally used. Then, some parser combinations that were developed in the same study are evaluated as well, assessing that more training data result in more accurate performances. Finally, to examine the impact that newly available tools have on parsing results, we train, test, and evaluate two neural parsers chosen among those best performing in the CoNLL 2018 Shared Task. Our experiments reach the highest accuracy rates achieved so far in automatic syntactic parsing of the  Index Thomisticus  Treebank and of Latin overall.}
}

@inproceedings{chr2021:Vauth,
    author    = {Michael Vauth and Hans Ole Hatzel and Evelyn Gius and Chris Biemann},
    title     = {Automated Event Annotation in Literary Texts},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper18.pdf},
    abstract  = {We approach the modeling of event structure of literary texts with narratological event concepts. A manually annotated corpus of 4 prose texts with event categories allows us to  learn to automatically classify events using a transformer model, relying on a rule-based system in conjunction with a pre-trained parser to identify events. For the evaluation of both manual and automated annotation, we use narrativity graphs, which capture the change in narrativity over the course of the text. In an exploratory analysis, we apply the event classifier in conjunction with graph-based narrativity metrics to a large literary corpus. We find that text length does neither influence the length of eventful passages nor the number of eventful passages in the beginnings of texts.}
}

@inproceedings{chr2021:Petitpierre,
    author    = {Rémi Petitpierre and Frédéric Kaplan and Isabella di Lenardo},
    title     = {Generic Semantic Segmentation of Historical Maps},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper27.pdf},
    abstract  = {Research in automatic map processing is largely focused on homogeneous corpora or even individual maps, leading to inflexible models. Based on two new corpora, the first one centered on maps of Paris and the second one gathering maps of cities from all over the world, we present a method for computing the figurative diversity of cartographic collections. In a second step, we discuss the actual opportunities for CNN-based semantic segmentation of historical city maps. Through several experiments, we analyze the impact of figurative and cultural diversity on the segmentation performance. Finally, we highlight the potential for large-scale and generic algorithms. Training data and code of the described algorithms are made open-source and published with this article.}
}

@inproceedings{chr2021:Tiihonen,
    author    = {Iiro Tiihonen and Mikko Tolonen and Leo Lahti},
    title     = {Probabilistic Analysis of Early Modern British Book Prices},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper9.pdf},
    abstract  = {Books are a valuable exception to the general rule that quantitative information about early modern history is scarce, as their survival rate during the period has varied between low and high tens of percents, and descriptive information summarizing their properties has been collected to library catalogues. However, one critical element that is essential for the numeric characterisation of a print product is most often missing - its price. In this paper, we use an exceptionally large data set of price information extracted from the English Short Title Catalogue (ESTC) for the early modern period to train a probabilistic model that predicts the price of a print product based on its physical properties. Our results suggest that just the simple physical properties of the print products can explain a significant proportion of the variation in prices. We use the model to quantitatively address the debated question about development of print product prices in eighteenth century Britain. We interpret the predictions of the model as a data driven narrative, and many of the developments it brings up can be readily linked with the relevant historical literature.}
}

@inproceedings{chr2021:Du,
    author    = {Keli Du and Julia Dudar and Cora Rok and Christof Schöch},
    title     = {Zeta  \&  Eta: An Exploration and Evaluation of Two Dispersion-based Measures of Distinctiveness},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper11.pdf},
    abstract  = {In Corpus Linguistics, numerous statistical measures have been adopted to analyze large amounts of textual data in a contrastive perspective, in order to extract characteristic or “distinctive” features. While the most widely-used keyness measures are based on word frequency, an increasing number of research papers recently suggested dispersion-based measures as a better solution. These, however, are not new to Computational Literary Studies (CLS). In 2007, John Burrows introduced Zeta, a statistical measure that is mainly based on the degree of dispersion of a feature in a text corpus. In this paper, we also introduce  Eta , a new measure of distinctiveness that is based on  deviation of proportions  suggested by Stefan Gries. By comparing Eta with Zeta, we demonstrate that both measures are able to identify relevant, interpretable distinctive words in a target corpus. Additionally, we make a first attempt to detect the key differences between these two measures by interpreting the top distinctive words.}
}

@inproceedings{chr2021:Ardanuy,
    author    = {Mariona Coll Ardanuy and Kaspar Beelen and Jon Lawrence and Katherine McDonough and Federico Nanni and Joshua Rhodes and Giorgia Tolfo and Daniel C.S. Wilson},
    title     = {Station to Station: Linking  and Enriching Historical British Railway Data},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper29.pdf},
    abstract  = {The transformative impact of the railway on nineteenth-century British society has been widely recognized, but understanding that process at scale remains challenging because the Victorian rail network was both vast and in a state of constant flux. Michael Quick's reference work  Railway Passenger Stations in Great Britain: a Chronology  offers a uniquely rich and detailed account of Britain's changing railway infrastructure. Its listing of over 12,000 stations allows us to reconstruct the coming of rail at both micro- and macro-scales; however, being published originally as a book, this resource was not well suited for systematic linking to other geographical data. This paper shows how such a minimally-structured historical directory can be transformed into an openly available structured and linked dataset, named  StopsGB (Structured Timeline of Passenger Stations in Great Britain), which will be of widespread interest across the historical, digital library and semantic web communities. To achieve this, we use traditional parsing techniques to convert the original document into a structured dataset of railway stations, with attributes containing information such as operating companies and opening and closing dates. We then identify a set of potential Wikidata candidates for each station using DeezyMatch, a deep neural approach to fuzzy string matching, and use a supervised classification approach to determine the best matching entity.}
}

@inproceedings{chr2021:Wevers,
    author    = {Melvin Wevers and Jan Kostkan and Kristoffer Nielbo},
    title     = {Event Flow - How Events Shaped the Flow of the News, 1950-1995},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper16.pdf},
    abstract  = {This article relies on information-theoretic measures to examine how events impacted the news for the period 1950-1995. Moreover, we present a method for event characterization in (unstructured) textual sources, offering a taxonomy of events based on the different ways they impacted the flow of news information. The results give us a better understanding of the relationship between events and their impact on news sources with varying ideological backgrounds.}
}

@inproceedings{chr2021:Schmidt,
    author    = {Thomas Schmidt and Christian Wolff},
    title     = {Exploring Multimodal Sentiment Analysis in Plays: A Case Study for a Theater Recording of Emilia Galotti},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper45.pdf},
    abstract  = {We present first results of an exploratory study about sentiment analysis via different media channels on a German historical play. We propose the exploration of other media channels than text for sentiment analysis on plays since the auditory and visual channel might offer important cues for sentiment analysis. We perform a  case study  and investigate how textual, auditory (voice-based), and visual (face-based) sentiment analysis perform compared to human annotations and how these approaches differ from each other. As use case we chose  Emilia Galotti  by the famous German playwright Gotthold Ephraim Lessing. We acquired a video recording of a 2002 theater performance of the play at the “Wiener Burgtheater”. We evaluate textual lexicon-based sentiment analysis and two state-of-the-art audio and video sentiment analysis tools. As gold standard we use speech-based annotations of three expert annotators. We found that the audio and video sentiment analysis do not perform better than the textual sentiment analysis and that the presentation of the video channel did not improve annotation statistics. We discuss the reasons for this negative result and limitations of the approaches. We also outline how we plan to further investigate the possibilities of multimodal sentiment analysis.}
}

@inproceedings{chr2021:Tambuscio,
    author    = {Marcella Tambuscio and Tara Lee Andrews},
    title     = {Geolocation and Named Entity Recognition in Ancient Texts: A Case Study about Ghewond's Armenian History},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper28.pdf},
    abstract  = {We present here a discussion about different methods to perform Named Entity Recognition tasks in order to extract geographic entities from the English translation of an Armenian text of the eighth century. Even though many tools are available and perform quite well with modern English, in this case they are only able to detect a very low percentage of the named geographic places. We compared four existing tools: NLTK and spaCy Python libraries, among the most used for NER tasks, TagMe, an entity linking tool that provide an annotation of found entities with Wikipedia pages, and Flair, a PyTorch library. We set these tools in order to select only geographical entities and we also tried two mixed methods: the best results on our data-set have been obtained by combining Flair and TagMe outputs with geographical clustering.}
}

@inproceedings{chr2021:Franceschet,
    author    = {Massimo Franceschet},
    title     = {The Sentiment of Crypto Art},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper10.pdf},
    abstract  = {Crypto art is a beautiful example of a side effect of a technology: blockchain. While commonly associated with decentralized finance and cryptocurrencies, blockchain technology allowed to create scarcity in a world -- digital art -- where everything can be infinitely duplicated and freely saved with a right-click of the mouse, forstering the growth, burst, and stabilization of a dizzying market made of artists, collectors, art galleries and curators. In this preliminary work we assess the sentiments expressed by crypto artists when they create art and those coveted by crypto collectors when they collect art. We find that artists communicate positive emotions like joy and trust instead of negative ones like fear and sadness. However, collectors are agnostic to these emotions. This might be useful information to integrate into a crypto art discovery system that we are currently building.}
}

@inproceedings{chr2021:Lauren,
    author    = {Lauren Fonteyn and Enrique Manjavacas Arévalo},
    title     = {Adjusting Scope: A Computational Approach to Case-Driven Research on Semantic Change},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper26.pdf},
    abstract  = {Computational studies of semantic change are often wide in scope, aiming to capture and quantify semantic change in language at large in a data-driven, `hands-off' way. Case-driven, corpus-linguistic studies of semantic change, by contrast, generally aim to tackle questions about the development of specific linguistic phenomena.   Due to its narrower scope, case-driven research is more restricted in terms of the data it may employ, and at the same time it requires a more fine-grained description of the targeted linguistic developments. As a result, case-driven studies face particular methodological challenges that are not at play in more wide-scoped approaches.  %   Because the two approaches differ with respect to flexibility in terms of the data they may employ to attain their goals, and the level of granularity with which developments are described, case-study driven research faces methodological challenges that are not at play in more wide-scoped approaches.    The aim of this paper is to set out a `hands-off' computational procedure to study specific cases of semantic change. The case we address is the development of the phrasal expression  to death  from a literal, resultative phrase (e.g.  he was beaten to death ) into an intensifier (e.g.  We were just pleased to death to see her ). We deploy hierarchical clustering algorithms over distributed meaning representations in order to capture the evolution of the semantic space of verbs that collocate with  to death . We then describe the arising diachronic processes by means of monotonic effects, providing a more accurate picture than customary linear regression models. The methodology we outline may help tackle some common challenges in the use of vector representations to study similar cases of semantic change. We end the discussion by pinpointing (remaining) challenges that case-driven research may encounter.}
}

@inproceedings{chr2021:Brottrager,
    author    = {Judith Brottrager and Annina Stahl and Arda Arslan},
    title     = {Predicting Canonization: Comparing Canonization Scores Based on Text-Extrinsic and -Intrinsic Features},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper21.pdf},
    abstract  = {The majority of literary texts ever written are hardly known, read, or studied today, and belong to the so-called  . Theories of canonization predominantly focus on sociocultural processes of selection which culminate in the formation of a canon, but say little about how the texts themselves contribute to canonization. In this paper, we propose an operationalization for canonization, which is then used to build a classifier that predicts a canonization score for a text by considering text-intrinsic features only. Working on a historical corpus of English and German texts, which includes both canonical and   works, the results show that a canonization score based on text-inherent features has weak correlations with a canonization score based on text-extrinsic features.}
}

@inproceedings{chr2021:Lang,
    author    = {Sarah Lang},
    title     = {Assessing Michael Maier's Contributions to Francis Anthony's Apologia (1616) Using Stylometry},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper44.pdf},
    abstract  = {This article attempts to shed new light on the collaboration between the chymical authors Michael Maier (1568--1622) and Francis Anthony (1550--1632) using stylometric authorship attribution. Maier and Anthony were friends and we know that they worked together on the English and Latin versions of Anthony's  Apologie  or  Apologia (1616) respectively. The question remains whether Maier was more than just a mere translator, as it has been claimed in the past -- notably by Maier himself. Using  R-Stylo , stylometric analyses are conducted. It is discussed what conclusions can be drawn from them given that we already know Maier and Anthony were working together and that Maier  was  the translator responsible for the Latin  Apologia (1616) ascribed to Anthony. In the end, stylometry doesn't offer enough evidence for us to make any definite claims regarding the authorship situation under discussion. It can, however, offer certain insights into the stylometric proximity between Maier and Anthony.}
}

@inproceedings{chr2021:Jiang,
    author    = {Ming Jiang and Yuerong Hu and Glen Worthey and Ryan C Dubnicek and Ted Underwood and J Stephen Downie},
    title     = {Impact of  OCR  Quality on  BERT  Embeddings in the Domain Classification of Book Excerpts},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper43.pdf},
    abstract  = {Digital humanities (DH) scholars have been increasingly interested in using BERT for document representation in computational text analysis. However, most word embeddings, including BERT embeddings, have been developed using ``clean" corpora, while DH research is usually based on digitized texts with optical character recognition (OCR) errors. Will these errors introduced by the digitization process reduce BERT's performance and distort the research findings? To shed light on the impact of OCR quality on BERT models, we conducted an empirical study on the resilience of BERT embeddings (pre-trained and fine-tuned) to OCR errors by measuring BERT's ability to enable classification of book excerpts by subject domain. We developed specialized parallel corpora for this task consisting of matching pairs of OCR’d text (19,049 volumes) and ``clean" re-keyed text (4,660 volumes) from English-language books in six  domains published from 1780 to 1993. This study is the first to systematically quantify OCR impact on contextualized word embedding techniques with a use case of OCR'd book datasets curated by digital libraries (DL). Experimental results show that pre-trained BERT is less robust when used on OCR'd texts; however, fine-tuning pre-trained BERT on OCR'd texts significantly improves its resilience to OCR noise in classification tasks according to the changes of classifier performance. These findings should assist DH scholars who are interested in using BERT for scholarly purposes.}
}

@inproceedings{chr2021:Moss,
    author    = {Fabian Moss and Maik Köster and Melinda Femminis and Coline Métrailler and François Bavaud},
    title     = {Digitizing a 19th-Century Music Theory Debate for Computational Analysis},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper31.pdf},
    abstract  = {We report the progress of the ongoing project      % \textbf{ANONYMIZED}.      ``Digitizing the Dualism Debate: a case study in the computational analysis of historical music theory sources''.      First, we give a brief introduction to the dualism debate, a central discussion in 19th-century German music theory. We then describe the transcription pipeline with which we process the digitized sources in order to arrive at a corpus of computationally feasible representations, and discuss a number of encountered challenges, e.g. the assignment of structural types and idiosyncratic symbols.     Employing text similarity measures and topic modeling, we present some preliminary analyses.      Future steps include text annotation, music encoding, and the presentation of the corpus with an online interface.}
}

@inproceedings{chr2021:Gabay,
    author    = {Simon Gabay},
    title     = {Beyond Idiolectometry? On Racine's Stylometric Signature},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper39.pdf},
    abstract  = {If stylometry has proven to be useful for literary history, especially for distant reading approaches of texts, it still has to show its efficiency regarding close reading. Taking the example of famous French playwright Jean Racine, we propose a double analysis of his plays, both distant and close, following the double objective of controlling its newly alleged paternity on Campistron's plays (which proves to be wrong using standard methods in stylometry), and interpreting the stylometric markers used for this attribution procedure. 17 th \, c. French having a relatively unstable spelling system, we also propose a new method for denoising, based on full linguistic annotation rather than simple lemmatisation.}
}

@inproceedings{chr2021:Baumann,
    author    = {Timo Baumann and Ashutosh Saboo},
    title     = {Evaluating Heuristics for Audio-Visual Translation},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper46.pdf},
    abstract  = {Dubbing, i.e., the lip-synchronous translation and revoicing of audio-visual media into a target language from a different source language, is essential for the full-fledged reception of foreign audio-visual media, be it movies, instructional videos or short social media clips.  In this paper, we objectify influences on the `dubbability' of translations,  i.e., how well a translation would be synchronously revoiceable to the lips on screen.  We explore the value of traditional heuristics used in evaluating the  qualitative  aspects , in particular matching bi \- labial consonants and the jaw opening while producing vowels, and control for  quantity , i.e., that translations are similar to the source in length. We perform an ablation study using an adversarial neural classifier  which is trained to differentiate ``true" dubbing translations from machine translations. While we are able to confirm the value of matching  lip closure  in dubbing, we find  that the opening angle of the jaw as determined by the realized vowel  may be less relevant than frequently considered in audio-visual translation.}
}

@inproceedings{chr2021:Wang,
    author    = {Haining Wang and Xin Xie and Allen Riddell},
    title     = {The Challenge of Vernacular and Classical Chinese Cross-Register Authorship Attribution},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper41.pdf},
    abstract  = {Ming-Qing fiction is widely regarded as the pinnacle of classical Chinese literature, but over three-quarters of vernacular fictional works were anonymously or pseudonymously composed, frustrating literary-historical research. To begin to address the problem, we propose a cross-register authorship attribution task: recover the authorship of a vernacular Chinese text given classical Chinese writing samples of known authorship. A corpus of eight authors known to have written in both registers was assembled to serve as a testbed. We describe the performance of models using different sets of function character/word frequencies as input features. This standard approach to authorship attribution performs well in the same-register setting but poorly in the cross-register setting. We discuss the degree of vernacularization and the amount of dialog in texts as key factors contributing to the low cross-register accuracy.}
}

@inproceedings{chr2021:Smits,
    author    = {Thomas Smits and Mike Kestemont},
    title     = {Towards Multimodal Computational Humanities. Using CLIP to Analyze Late-Nineteenth Century Magic Lantern Slides},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper23.pdf},
    abstract  = {The introduction of the CLIP model signaled a breakthrough in multimodal deep learning. This paper examines whether CLIP can be fruitfully applied to a (binary) classification task in the Humanities. We focus on a historical collection of late-nineteenth century magic lantern slides from the Lucerna database. Based on the available metadata, we evaluate CLIP’s performance on classifying slide images into `exterior' and `interior' categories. We compare the performance of several textual prompts for CLIP to two conventional mono-modal models (textual and visual) which we train and evaluate on the same stratified set of 5,244 magic lantern slides and their captions. We find that the textual and multimodal models achieve a respectable performance ( 0.80 accuracy) but are still outperformed by a vision model that was fine-tuned to the task ( 0.89). We flag three methodological issues that might arise from the application of CLIP in the (computational) humanities. First, the lack of (need for) labelled data makes it hard to inspect and/or interpret the performance of the model. Second, CLIP's zero-shot capability only allows for classification tasks to be simulated, which makes it doubtful if standard metrics can be used to compare its performance to text and/or image models. Third, the lack of effective prompt engineering techniques makes the performance of CLIP (highly) unstable.}
}

@inproceedings{chr2021:Hellwig,
    author    = {Oliver Hellwig and Sven Sellmer and Sebastian Nehrdich},
    title     = {Obtaining More Expressive Corpus Distributions for Standardized Ancient Languages},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper4.pdf},
    abstract  = {This paper introduces a latent variable model for ancient languages that aims at quantifying the influence that early authoritative works exert on  their literary successors in terms of lexis. The model jointly estimates the amount of word reuse, based on uni- and bigrams of words, and the date of composition of each text. We apply the model to a corpus of pre-Renaissance Latin texts composed between the 3rd c.}
}

@inproceedings{chr2021:Lee,
    author    = {Benjamin Lee and Joshua Ortiz Baco and Sarah Salter and Jim Casey},
    title     = {Navigating the Mise-en-Page: Interpretive Machine Learning Approaches to the Visual Layouts of Multi-Ethnic Periodicals},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper3.pdf},
    abstract  = {This paper presents a computational method of analysis that draws from machine learning, library science, and literary studies to map the visual layouts of multi-ethnic newspapers from the late 19th and early 20th century United States. This work departs from prior approaches to newspapers that focus on individual pieces of textual and visual content. Our method combines Chronicling America's MARC data and the Newspaper Navigator machine learning dataset to identify the visual patterns of newspaper page layouts. By analyzing high-dimensional visual similarity, we aim to better understand how editors spoke and protested through the layout of their papers.}
}

@inproceedings{chr2021:Kase,
    author    = {Vojtech Kase and Petra Heřmánková and Adéla Sobotková},
    title     = {Classifying Latin Inscriptions of the Roman Empire: A Machine-Learning Approach},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/short_paper12.pdf},
    abstract  = {Large-scale synthetic research in ancient history is often hindered by the incompatibility of taxonomies used by different digital datasets. Using the example of enriching the Latin Inscriptions from the Roman Empire dataset (LIRE), we demonstrate that machine-learning classification models can bridge the gap between two distinct classification systems and make comparative study possible. We report on training, testing and application of a machine learning classification model using inscription categories from the Epigraphic Database Heidelberg (EDH) to label inscriptions from the Epigraphic Database Claus-Slaby (EDCS). The model is trained on a labeled set of records included in both sources ( N =46,171). Several different classification algorithms and parametrizations are explored. The final model is based on Extremely Randomized Trees algorithm (ET) and employs 10,055 features, based on several attributes. The final model classifies two thirds of a test dataset with 98 \%  accuracy and 85 \%  of it with 95 \%  accuracy. After model selection and evaluation, we apply the model on inscriptions covered exclusively by EDCS ( N =83,482) in an attempt to adopt one consistent system of classification for all records within the LIRE dataset.}
}

@inproceedings{chr2021:Piper,
    author    = {Andrew Piper and Sunyam Bagga and Laura Monteiro and Andrew Yang and Marie Labrosse and Yu Lu Liu},
    title     = {Detecting Narrativity Across Long Time Scales},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper49.pdf},
    abstract  = {Storytelling is a universal human practice that serves as a key site of education, collective memory, fostering social belief systems, and furthering human creativity. It can occur in different discursive domains for different social purposes with differing degrees of intensity. In this project, we develop computational methods for measuring the degree of narrativity in over 335,000 text passages distributed across two- to three-hundred years of history and four separate discursive domains (fiction, non-fiction, science, and poetry). We show how these domains are strongly differentiated according to their degree of narrative communication and, second, how truth-based discourse has declined considerably in its utilization of narrative communication. These findings suggest that there has been a long-term historical differentiation between the practices of knowing and telling, which raises important questions with respect to the social acceptance of both science and the arts.}
}

@inproceedings{chr2021:Baas,
    author    = {Jurian Baas and Mehdi Dastani and Ad Feelders},
    title     = {Entity Matching in Digital Humanities Knowledge Graphs},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper5.pdf},
    abstract  = {We propose a method for entity matching that takes into account the characteristic complex properties of decentralized cultural heritage data sources, where multiple data sources may contain duplicates within and between sources. We apply the proposed method to historical data from the Amsterdam City Archives using several clustering algorithms and evaluate the results against a partial ground truth. We also evaluate our method on a semi-synthetic data set for which we have a complete ground truth. The results show that the proposed method for entity matching performs well and is able to handle the complex properties of historical data sources.}
}

@inproceedings{chr2021:Crepel,
    author    = {Maxime Crepel and Salomé Do and Jean-Philippe Cointet and Dominique Cardon and Yannis Bouachera},
    title     = {Mapping AI Issues in Media Through NLP Methods},
    booktitle = {Proceedings of the Conference on Computational Humanities Research 2021 (CHR 2021)},
    date      = {17--19 November 2021},
    place     = {Amsterdam, The Netherlands},
    publisher = {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
    volume    = {Vol-2989},
    url       = {http://ceur-ws.org/Vol-2989/long_paper22.pdf},
    abstract  = {Using a variety of NLP methods on a corpus of press articles, we show that there are two dominant regimes of criticism of artificial intelligence that coexist within the media sphere. Combining text classification algorithms to detect critical articles and a topological analysis of the terms extracted from the corpus, we reveal two semantic spaces, involving different technological and human entities, but also distinct temporality and issues. On the one hand,  the algorithms that shape our daily computing environments are associated with a critical discourse on bias, discrimination, surveillance, censorship and amplification phenomena in the spread of inappropriate content. On the other hand, robots and AI, which refer to autonomous and embodied technical entities, are associated with a prophetic discourse alerting us to our ability to control these agents that simulate or exceed our physical and cognitive capacities and threaten our physical security or our economic model.}
}

